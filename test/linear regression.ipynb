{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_removal(original_linear_regressor, X_train, y_train, x_test, max_size):\n",
    "    # Initialize variables to keep track of the best subset and loss difference for parameter changes\n",
    "    best_subset_fix_test = np.full((max_size), None)\n",
    "\n",
    "    original_score = x_test.dot(original_linear_regressor.coef_) + original_linear_regressor.intercept_\n",
    "\n",
    "    # Loop over different subset sizes from 1 to max_size\n",
    "    for subset_size in range(1, max_size + 1):\n",
    "        # Generate all combinations of subsets of the current size\n",
    "        subset_combinations = combinations(range(X_train.shape[0]), subset_size)\n",
    "\n",
    "        max_score_difference = -float(\"inf\")\n",
    "\n",
    "        for subset_to_remove in subset_combinations:\n",
    "            # Create a new training set without the selected data points\n",
    "            reduced_X_train = np.delete(X_train, subset_to_remove, axis=0)\n",
    "            reduced_y_train = np.delete(y_train, subset_to_remove, axis=0)\n",
    "\n",
    "            # Train a Linear Regression model on the reduced training set\n",
    "            reduced_linear_regressor = LinearRegression()\n",
    "            reduced_linear_regressor.fit(reduced_X_train, reduced_y_train)\n",
    "\n",
    "            # Make inference\n",
    "            reduced_score = x_test.dot(reduced_linear_regressor.coef_) + reduced_linear_regressor.intercept_\n",
    "\n",
    "            # Calculate the difference in loss\n",
    "            score_difference = reduced_score - original_score\n",
    "\n",
    "            # Update if the current subset induces the minimum change in loss\n",
    "            if score_difference > max_score_difference:\n",
    "                max_score_difference = score_difference\n",
    "                best_subset_fix_test[subset_size - 1] = subset_to_remove\n",
    "\n",
    "        print(f\"Best subset of size {subset_size}: {best_subset_fix_test[subset_size - 1]}\")\n",
    "\n",
    "    return best_subset_fix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influence(X_train, y_train, coef, x_test, leverage=False):\n",
    "\tn_samples = X_train.shape[0]\n",
    "\tinfluences = np.zeros(n_samples)\n",
    "\tX = np.hstack((np.ones((n_samples, 1)), X_train))  # Add a column of ones for the intercept\n",
    "\n",
    "\t# Fixed test point with intercept feature\n",
    "\tfixed_test_point = np.hstack((1, x_test))\n",
    "\n",
    "\tN = np.dot(X.T, X)\n",
    "\tN_inv = np.linalg.inv(N)\n",
    "\n",
    "\tinfluences = np.dot(np.dot(fixed_test_point, N_inv), X.T * (np.dot(X, coef) - y_train))\n",
    "\n",
    "\tif leverage:\n",
    "\t\tH = X.dot(N_inv).dot(X.T)\n",
    "\t\t# Calculate the influence using the provided formula\n",
    "\t\tinfluences = influences / (1 - np.diagonal(H))\n",
    "\n",
    "\t\tprint(f\"Largest off-diagonal entry of H: {H[~np.eye(*H.shape, dtype=bool)].max()}\")\n",
    "\n",
    "\t\t# Plot the leverage scores empirical distribution\n",
    "\t\tplt.figure(figsize=(12, 6))  # Set the figure size to accommodate both plots side by side\n",
    "\t\tplt.subplot(1, 2, 1)  # Create the first subplot\n",
    "\t\tplt.hist(np.diagonal(H), bins=30, color='blue', alpha=0.7)  # Adjust appearance as needed\n",
    "\t\tplt.xlabel('Leverage Score')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\t\tplt.title('Leverage Scores Empirical Distribution')\n",
    "\n",
    "\t\t# Create the second subplot for off-diagonal entries of H\n",
    "\t\tplt.subplot(1, 2, 2)\n",
    "\t\toff_diagonal_H = H - np.diag(np.diagonal(H))  # Extract off-diagonal entries\n",
    "\t\tplt.hist(off_diagonal_H[~np.eye(*H.shape, dtype=bool)], bins=30, color='green', alpha=0.7)  # Adjust appearance as needed\n",
    "\t\tplt.xlabel('Off-Diagonal Entry of H')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\t\tplt.title('Off-Diagonal Entries of H Empirical Distribution')\n",
    "\n",
    "\t\tplt.tight_layout()  # Adjust spacing between subplots for a clean layout\n",
    "\t\tplt.show()  # Display both plots side-by-side\n",
    "\n",
    "\treturn influences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, n, max_size):\n",
    "\tprint(f\"Dataset size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "\ttest_size = 1 - n / X.shape[0]\n",
    "\n",
    "\t# Split the data into training and testing sets\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "\t# Create a Linear Regression model\n",
    "\tlinear_regressor = LinearRegression()\n",
    "\n",
    "\t# Train the linear regressor on the training data\n",
    "\tlinear_regressor.fit(X_train, y_train)\n",
    "\n",
    "\t# Make predictions on the test data\n",
    "\ty_pred = linear_regressor.predict(X_test)\n",
    "\n",
    "\t# Calculate Mean Squared Error (MSE)\n",
    "\tmse = np.mean((y_pred - y_test) ** 2)\n",
    "\tprint(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "\tfixed_test_point_index = 0\n",
    "\tfixed_test_point = X_test[fixed_test_point_index]\n",
    "\n",
    "\tbest_subset = brute_force_removal(linear_regressor, X_train, y_train, fixed_test_point, max_size)\n",
    "\n",
    "\t# Create the logistic regression model and fit it\n",
    "\tcoefficients = np.hstack((linear_regressor.intercept_, linear_regressor.coef_))\n",
    "\n",
    "\n",
    "\tprint(\"Without leverage\")\n",
    "\tinfluences = influence(X_train, y_train, coefficients, fixed_test_point)\n",
    "\ttop_indices = np.argsort(influences)[-(max_size+3):][::-1]\n",
    "\tprint(f\"Top {max_size+3} Influential Data Points Estimate: {top_indices}\")\n",
    "\n",
    "\n",
    "\tprint(\"With leverage\")\n",
    "\tinfluences = influence(X_train, y_train, coefficients, fixed_test_point, leverage=True)\n",
    "\ttop_indices = np.argsort(influences)[-(max_size+3):][::-1]\n",
    "\tprint(f\"Top {max_size+3} Influential Data Points Estimate: {top_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 442 samples, 10 features\n",
      "Mean Squared Error: 3423.4462\n",
      "Best subset of size 1: (10,)\n",
      "Best subset of size 2: (10, 69)\n",
      "Best subset of size 3: (10, 12, 69)\n"
     ]
    }
   ],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "linear_regression(X, y, 100, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
