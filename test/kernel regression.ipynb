{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from itertools import combinations\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression(X_train, y_train, x_test, kernel, alpha):\n",
    "    kernel_regressor = KernelRidge(alpha=alpha, kernel=kernel)\n",
    "    kernel_regressor.fit(X_train, y_train)\n",
    "    y_pred = kernel_regressor.predict(x_test.reshape(1, -1))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_removal(X_train, y_train, x_test, max_size, kernel, alpha):\n",
    "    # Initialize variables to keep track of the best subset and loss difference for parameter changes\n",
    "    best_subset_fix_test = np.full((max_size), None)\n",
    "    \n",
    "    original_score = kernel_regression(X_train, y_train, x_test, kernel, alpha)\n",
    "    \n",
    "    # Loop over different subset sizes from 1 to max_size\n",
    "    for subset_size in range(1, max_size + 1):\n",
    "        # Generate all combinations of subsets of the current size\n",
    "        subset_combinations = combinations(range(X_train.shape[0]), subset_size)\n",
    "        \n",
    "        max_score_difference = -float(\"inf\")\n",
    "        \n",
    "        for subset_to_remove in subset_combinations:\n",
    "            # Create a new training set without the selected data points\n",
    "            reduced_X_train = np.delete(X_train, subset_to_remove, axis=0)\n",
    "            reduced_y_train = np.delete(y_train, subset_to_remove, axis=0)\n",
    "            \n",
    "            # Calculate the influence using kernel regression\n",
    "            reduced_score = kernel_regression(reduced_X_train, reduced_y_train, x_test, kernel, alpha)\n",
    "            \n",
    "            # Calculate the difference in loss\n",
    "            score_difference = reduced_score - original_score\n",
    "            \n",
    "            # Update if the current subset induces the minimum change in loss\n",
    "            if score_difference > max_score_difference:\n",
    "                max_score_difference = score_difference\n",
    "                best_subset_fix_test[subset_size - 1] = subset_to_remove\n",
    "        \n",
    "        print(f\"Best subset of size {subset_size}: {best_subset_fix_test[subset_size - 1]}\")\n",
    "    \n",
    "    return best_subset_fix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def influence(X_train, y_train, x_test, kernel, alpha):\n",
    "    n_samples = X_train.shape[0]\n",
    "    influences = np.zeros(n_samples)\n",
    "    \n",
    "    kernel_regressor = KernelRidge(alpha=alpha, kernel=kernel)\n",
    "    kernel_regressor.fit(X_train, y_train)\n",
    "    y_pred = kernel_regressor.predict(x_test.reshape(1, -1))\n",
    "    \n",
    "    # K = kernel(X_train, X_train) + alpha * np.identity(n_samples)\n",
    "    \n",
    "    # K_inv = np.linalg.inv(K)\n",
    "    \n",
    "    # k = kernel(X_train, x_test)\n",
    "    \n",
    "    # # Calculate the influence using the provided formula\n",
    "    # influences = np.dot(K_inv, k)\n",
    "    \n",
    "    # # Calculate the leverage scores using the provided formula\n",
    "    # H = np.dot(K_inv, K)\n",
    "    # influences = influences / (1 - np.diagonal(H))\n",
    "    \n",
    "    print(f\"Largest off-diagonal entry of H: {H[~np.eye(*H.shape, dtype=bool)].max()}\")\n",
    "    \n",
    "    # Plot the leverage scores empirical distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(np.diagonal(H), bins=30, color='blue', alpha=0.7)\n",
    "    plt.xlabel('Leverage Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Leverage Scores Empirical Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    off_diagonal_H = H - np.diag(np.diagonal(H))\n",
    "    plt.hist(off_diagonal_H[~np.eye(*H.shape, dtype=bool)], bins=30, color='green', alpha=0.7)\n",
    "    plt.xlabel('Off-Diagonal Entry of H')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Off-Diagonal Entries of H Empirical Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return influences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression_and_influence(X, y, n, max_size, kernel, alpha):\n",
    "    print(f\"Dataset size: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "    \n",
    "    test_size = 1 - n / X.shape[0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    best_subset = brute_force_removal(X_train, y_train, X_test[0], max_size, kernel, alpha)\n",
    "    print(f\"Best subset: {best_subset}\")\n",
    "    \n",
    "    influences = influence(X_train, y_train, X_test[0], kernel, alpha)\n",
    "    top_indices = np.argsort(influences)[-(max_size+3):][::-1]\n",
    "    print(f\"Top {max_size+3} Influential Data Points Estimate: {top_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n",
    "    \n",
    "# Parameters for kernel regression\n",
    "kernel = 'rbf'  # You can use other kernel types like 'linear', 'poly', 'sigmoid', etc.\n",
    "alpha = 1.0  # Regularization parameter\n",
    "\n",
    "# Set the values of n and max_size\n",
    "n = 50\n",
    "max_size = 5\n",
    "\n",
    "# Call the main function to perform kernel regression and influence analysis\n",
    "kernel_regression_and_influence(X, y, n, max_size, kernel, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
