{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from itertools import combinations\n",
    "\n",
    "from IWLS import IWLS, adaptive_IWLS\n",
    "from first_order import first_order\n",
    "from margin import margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general parameters\n",
    "n, k = 50, 5\n",
    "job_n = 50\n",
    "\n",
    "seed = 22\n",
    "cov = 0.5\n",
    "\n",
    "target = \"test_loss\"\n",
    "np.random.seed(seed)\n",
    "\n",
    "# generate data\n",
    "mean_n = np.array([-1, 0])\n",
    "mean_p = np.array([1, 0])\n",
    "covariance = np.eye(2) * cov\n",
    "x_n = np.random.multivariate_normal(mean_n, covariance, int(n/2))\n",
    "x_p = np.random.multivariate_normal(mean_p, covariance, int(n/2))\n",
    "\n",
    "y_n = np.zeros(int(n/2)) # 0 labels\n",
    "y_p = np.ones(int(n/2))  # 1 labels\n",
    "\n",
    "X_train = np.vstack((x_n, x_p))\n",
    "y_train = np.hstack((y_n, y_p))\n",
    "\n",
    "# Choose mean_n or mean_p w.p. 1/2\n",
    "if np.random.rand() < 0.5:\n",
    "\tx_test = np.random.multivariate_normal(mean_n, covariance)\n",
    "\ty_test = 0\n",
    "else:\n",
    "\tx_test = np.random.multivariate_normal(mean_p, covariance)\n",
    "\ty_test = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_effect(X_train, y_train, x_test, y_test, subset_to_remove, original_score, target=\"probability\"):\n",
    "    # Train a Logistic Regression classifier on the reduced training set\n",
    "    reduced_X_train = np.delete(X_train, subset_to_remove, axis=0)\n",
    "    reduced_y_train = np.delete(y_train, subset_to_remove, axis=0)\n",
    "    reduced_lr = LogisticRegression(penalty=None).fit(reduced_X_train, reduced_y_train)\n",
    "\n",
    "    # Make inference\n",
    "    if target == \"probability\":\n",
    "        reduced_score = reduced_lr.predict_proba(x_test.reshape(1, -1))[0][1]\n",
    "    elif target == \"train_loss\":\n",
    "        reduced_score = log_loss(reduced_y_train, reduced_lr.predict_proba(reduced_X_train), labels=[0, 1])\n",
    "    elif target == \"test_loss\":\n",
    "        reduced_score = log_loss([y_test], reduced_lr.predict_proba(x_test.reshape(1, -1)), labels=[0, 1])\n",
    "  \n",
    "    # Calculate the difference in predicted probabilities\n",
    "    score_difference = reduced_score - original_score\n",
    "\n",
    "    return score_difference\n",
    "\n",
    "# The actual effect of a specific k, not <= k\n",
    "def actual(X_train, y_train, x_test, y_test, k=10, job_n=50, target=\"probability\"):\n",
    "    # Create a Logistic Regression classifier\n",
    "    original_lr = LogisticRegression(penalty=None).fit(X_train, y_train)\n",
    " \n",
    "    # Initialize variables to keep track of the best subset and loss difference for parameter changes\n",
    "    best_subset = np.full((k), None)\n",
    "    best_subset_combination = []\n",
    "    \n",
    "    if target == \"probability\":\n",
    "        original_score = original_lr.predict_proba(x_test.reshape(1, -1))[0][1] # We're looking at the predicted probability of the positive class\n",
    "    elif target == \"train_loss\":\n",
    "        original_score = log_loss(y_train, original_lr.predict_proba(X_train), labels=[0, 1])\n",
    "    elif target == \"test_loss\":\n",
    "        original_score = log_loss([y_test], original_lr.predict_proba(x_test.reshape(1, -1)), labels=[0, 1])\n",
    "\n",
    "    # Loop over different subset sizes from 1 to k\n",
    "    for subset_size in range(1, k + 1):\n",
    "        # Generate all combinations of subsets of the current size\n",
    "        subset_combinations = combinations(range(X_train.shape[0]), subset_size)\n",
    "        combinations_list = list(combinations(range(X_train.shape[0]), subset_size))\n",
    "  \n",
    "        scores = Parallel(n_jobs=job_n)(delayed(actual_effect)(X_train, y_train, x_test, y_test, subset_to_remove, original_score, target) for subset_to_remove in subset_combinations)\n",
    "    \n",
    "        sort_subset_combinations = np.array(combinations_list)[np.argsort(scores)[::-1]]\n",
    "        best_subset_combination.append(sort_subset_combinations)\n",
    "        best_subset[subset_size - 1] = sort_subset_combinations[0]\n",
    "\n",
    "    return [scores, best_subset_combination, best_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, best_subset_combination, best_subset = actual(X_train, y_train, x_test, y_test, k=k, job_n=job_n, target=target)\n",
    "best_k_subset = best_subset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IWLS_best = IWLS(X_train, y_train, x_test, y_test, target=target)\n",
    "adaptive_IWLS_best_k = adaptive_IWLS(X_train, y_train, x_test, y_test, k=k, target=target)\n",
    "ind_n, ind_p = margin(X_train, y_train)\n",
    "FO_best = first_order(X_train, y_train, x_test, y_test, target=target)\n",
    "best_k_subset_combination = best_subset_combination[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_lr = LogisticRegression(penalty=None).fit(X_train, y_train)\n",
    "\n",
    "if target == \"probability\":\n",
    "        original_score = original_lr.predict_proba(x_test.reshape(1, -1))[0][1] # We're looking at the predicted probability of the positive class\n",
    "elif target == \"train_loss\":\n",
    "    original_score = log_loss(y_train, original_lr.predict_proba(X_train), labels=[0, 1])\n",
    "elif target == \"test_loss\":\n",
    "    original_score = log_loss([y_test], original_lr.predict_proba(x_test.reshape(1, -1)), labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
