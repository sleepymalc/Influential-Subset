{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from actual import actual_effect\n",
    "from IWLS import IWLS, adaptive_IWLS\n",
    "from first_order import first_order, adaptive_first_order\n",
    "from margin import margin\n",
    "\n",
    "from target import target_value\n",
    "from utility import data_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "d = 2\n",
    "k = 5\n",
    "seeds = [1, 21, 41, 61, 81, 2, 22, 42, 62, 82, 3, 23, 43, 63, 83, 4, 24, 44, 64, 84]\n",
    "covs = [0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "targets = [\"probability\", \"abs_probability\", \"test_loss\", \"abs_test_loss\", \"avg_abs_test_loss\", \"abs_avg_test_loss\"]\n",
    "\n",
    "target = targets[1]\n",
    "\n",
    "methods = [\"IWLS\", \"Adaptive IWLS\", \"Margin-based\", \"First-order\", \"Adaptive First-order\"]\n",
    "num_methods, num_covs, num_seeds = len(methods), len(covs), len(seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_actual_ranks(file_content):\n",
    "    general_ranks = re.findall(r'Actual rank: (\\d+)', file_content)\n",
    "    result = {\n",
    "        \"IWLS\": int(general_ranks[0]),\n",
    "        \"Adaptive IWLS\": int(general_ranks[1]),\n",
    "        \"Margin-based\": min(int(general_ranks[2]), int(general_ranks[3])),\n",
    "        \"First-order\": int(general_ranks[4]), \n",
    "        \"Adaptive First-order\": int(general_ranks[5])\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def read_best_k_subset(file_path):\n",
    "    # Initialize an empty list to store the best k subset\n",
    "    best_k_subset = []\n",
    "\n",
    "    # Open the file and read line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Use regular expressions to find and extract the best k subset\n",
    "    pattern = re.compile(fr'size {k}: \\[([\\d\\s]+)\\]')\n",
    "    for line in lines:\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            # Extract numbers from the matched group and convert to a list of integers\n",
    "            numbers_str = match.group(1)\n",
    "            best_k_subset = list(map(int, numbers_str.split()))\n",
    "\n",
    "            # Break the loop once the information is found\n",
    "            break\n",
    "\n",
    "    return best_k_subset\n",
    "\n",
    "# ratios w.r.t. ground-truth\n",
    "def ratio_ratio_per_seed_cov(seed, cov, target):\n",
    "    X_train, y_train, X_test, y_test = data_generation(n, d, cov, seed, target=target)\n",
    "    \n",
    "    original_value = target_value(X_train, y_train, X_test, y_test, target=target)\n",
    "    \n",
    "    ind_n, ind_p = margin(X_train, y_train)\n",
    "    \n",
    "    scores = np.array([\n",
    "        actual_effect(X_train, y_train, X_test, y_test, read_best_k_subset(f'results/target={target}/n={n}_d={d}_k={k}/s={seed}_cov={cov}.txt'), original_value, target=target), \n",
    "        actual_effect(X_train, y_train, X_test, y_test, IWLS(X_train, y_train, X_test, y_test, target=target)[:k], original_value, target=target), \n",
    "        actual_effect(X_train, y_train, X_test, y_test, adaptive_IWLS(X_train, y_train, X_test, y_test, k=k, target=target), original_value, target=target),\n",
    "        max(actual_effect(X_train, y_train, X_test, y_test, ind_n[:k], original_value, target=target), actual_effect(X_train, y_train, X_test, y_test, ind_p[:k], original_value, target=target)),\n",
    "        actual_effect(X_train, y_train, X_test, y_test, first_order(X_train, y_train, X_test, y_test, target=target)[:k], original_value, target=target),\n",
    "        actual_effect(X_train, y_train, X_test, y_test, adaptive_first_order(X_train, y_train, X_test, y_test, k=k, target=target), original_value, target=target)\n",
    "    ])\n",
    "    \n",
    "    ratio = {\n",
    "        \"IWLS\": scores[1] / scores[0],\n",
    "        \"Adaptive IWLS\": scores[2] / scores[0],\n",
    "        \"Margin-based\": scores[3] / scores[0],\n",
    "        \"First-order\": scores[4] / scores[0],\n",
    "        \"Adaptive First-order\": scores[5] / scores[0],\n",
    "    }\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "# ranks.shape = (num_methods, num_experiments)\n",
    "def Borda_count(ranks, weights=[5, 4, 3, 2, 1]):\n",
    "    num_methods, num_experiments = ranks.shape\n",
    "\n",
    "    weighted_borda_count = np.zeros((num_methods, num_experiments), dtype=int)\n",
    "\n",
    "    # Calculate weighted Borda count for each seed and covariance\n",
    "    for experiment_idx in range(num_experiments):\n",
    "        # Sort indices based on actual ranks for the current experiment\n",
    "        # tie-handling. ref: https://stackoverflow.com/questions/39059371/can-numpys-argsort-give-equal-element-the-same-rank\n",
    "        def rankmin(x):\n",
    "            u, inv, counts = np.unique(x, return_inverse=True, return_counts=True)\n",
    "            csum = np.zeros_like(counts)\n",
    "            csum[1:] = counts[:-1].cumsum()\n",
    "            return csum[inv]\n",
    "\n",
    "        sorted_indices = rankmin(ranks[:, experiment_idx])\n",
    "\n",
    "        # Assign weighted Borda count scores\n",
    "        for method_idx, rank in enumerate(sorted_indices):\n",
    "            weighted_borda_count[method_idx, experiment_idx] = weights[rank]\n",
    "            \n",
    "    total_weighted_borda_count = weighted_borda_count.sum(axis=1)\n",
    "    ranked_methods_weighted = np.argsort(total_weighted_borda_count)[::-1]\n",
    "    \n",
    "    print(f\"\\nWeighted Borda Count Rankings (target={target}):\")\n",
    "    for rank, method_idx in enumerate(ranked_methods_weighted):\n",
    "        method_name = methods[method_idx]\n",
    "        print(f\"{rank + 1}. {method_name}: {total_weighted_borda_count[method_idx]}\")\n",
    "\n",
    "    return total_weighted_borda_count\n",
    "\n",
    "# ranks.shape = (num_methods, num_experiments)\n",
    "def hitting_rate(ranks, rank_range=(1, 1)):\n",
    "    num_methods, num_experiments = ranks.shape\n",
    "\n",
    "    hitting_rates = np.zeros(num_methods, dtype=float)\n",
    "\n",
    "    for method_idx in range(num_methods):\n",
    "        total_hits = 0\n",
    "        for experiment_idx in range(num_experiments):\n",
    "            rank = ranks[method_idx, experiment_idx]\n",
    "\n",
    "            if rank_range[1] == rank_range[0]:\n",
    "                total_hits += 1 if rank == rank_range[0] else 0\n",
    "            else:\n",
    "                total_hits += 1 if rank in range(rank_range[0], rank_range[1] + 1) else 0\n",
    "\n",
    "        hitting_rates[method_idx] = total_hits / num_experiments\n",
    "\n",
    "    ranked_methods_hitting = np.argsort(hitting_rates)[::-1]\n",
    "\n",
    "    # Display hitting rates\n",
    "    print(f\"\\nHitting Rates (range={rank_range}, target={target}):\")\n",
    "    for rank, method_idx in enumerate(ranked_methods_hitting):\n",
    "        method_name = methods[method_idx]\n",
    "        print(f\"{rank + 1}. {method_name}: {hitting_rates[method_idx]}\")\n",
    "\n",
    "    return hitting_rates\n",
    "\n",
    "# ratios.shape = (num_methods, num_experiments)\n",
    "def average_ratio(ratios):\n",
    "    avg_ratios = ratios.mean(axis=1)\n",
    "    \n",
    "    # Rank methods based on average ratios\n",
    "    ranked_methods_avg_ratio = np.argsort(avg_ratios)[::-1]\n",
    "\n",
    "    # Display the ranking based on weighted Borda count scores\n",
    "    print(f\"\\nAverage Ratios (w.r.t. ground-truth) Rankings (target={target}):\")\n",
    "    for rank, method_idx in enumerate(ranked_methods_avg_ratio):\n",
    "        method_name = methods[method_idx]\n",
    "        print(f\"{rank + 1}. {method_name}: {avg_ratios[method_idx]}\")\n",
    "\n",
    "    return avg_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(metric=\"ratio\"):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))  # Create a 2x3 grid of subplots\n",
    "\n",
    "    for target_idx, target in enumerate(targets):\n",
    "        print(f\"Target: {target}\")\n",
    "        rank_array = np.zeros((num_seeds, num_covs, num_methods), dtype=int)\n",
    "\n",
    "        # Process each file and populate the array\n",
    "        for seed_idx, seed in enumerate(seeds):\n",
    "            for cov_idx, cov in enumerate(covs):\n",
    "                file_path = f'results/target={target}/n={n}_d={d}_k={k}/s={seed}_cov={cov}.txt'\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_content = file.read()\n",
    "                    actual_ranks = read_actual_ranks(file_content)\n",
    "                    # Populate the array\n",
    "                    for method_idx, method_name in enumerate(methods):\n",
    "                        rank_array[seed_idx, cov_idx, method_idx] = actual_ranks.get(method_name, 0)  # Default to 0 if method not found\n",
    "\n",
    "        ratio_array = np.zeros((num_seeds, num_covs, num_methods), dtype=float)\n",
    "        # Process each seed and covariance and populate the array\n",
    "        for seed_idx, seed in enumerate(seeds):\n",
    "            for cov_idx, cov in enumerate(covs):\n",
    "                ratio = ratio_ratio_per_seed_cov(seed, cov)\n",
    "                for method_idx, method_name in enumerate(methods):\n",
    "                    ratio_array[seed_idx, cov_idx, method_idx] = ratio.get(method_name, 0)  # Default to 0 if method not found\n",
    "\n",
    "        rank_method_cov_seed = rank_array.swapaxes(0, 2) # method, cov, seed\n",
    "        ratio_method_cov_seed = ratio_array.swapaxes(0, 2) # method, cov, seed\n",
    "        rank_cov_method_seed = rank_method_cov_seed.swapaxes(0, 1) # cov, method, seed\n",
    "        ratio_cov_method_seed = ratio_method_cov_seed.swapaxes(0, 1) # cov, method, seed\n",
    "\n",
    "        if metric == \"Borda\":    \n",
    "            result = np.array(Parallel(n_jobs=50)(delayed(Borda_count)(rank_cov_method_seed[cov_idx]) for cov_idx in range(num_covs)))\n",
    "        elif metric == \"hitting_rate_1\":\n",
    "            result = np.array(Parallel(n_jobs=50)(delayed(hitting_rate)(rank_cov_method_seed[cov_idx], rank_range=(1, 1)) for cov_idx in range(num_covs)))\n",
    "        elif metric == \"hitting_rate_1_10\":\n",
    "            result = np.array(Parallel(n_jobs=50)(delayed(hitting_rate)(rank_cov_method_seed[cov_idx], rank_range=(1, 10)) for cov_idx in range(num_covs)))\n",
    "        elif metric == \"ratio\":\n",
    "            result = np.array(Parallel(n_jobs=50)(delayed(average_ratio)(ratio_cov_method_seed[cov_idx]) for cov_idx in range(num_covs)))\n",
    "        \n",
    "\n",
    "        # Plot in the corresponding subplot\n",
    "        row_idx, col_idx = divmod(target_idx, 3)  # Calculate subplot index\n",
    "        for method_idx, method_name in enumerate(methods):\n",
    "            axs[row_idx, col_idx].plot(covs, result[:, method_idx], label=method_name)\n",
    "            \n",
    "        axs[row_idx, col_idx].set_title(f'Target={target}')\n",
    "        axs[row_idx, col_idx].set_xlabel('Covariance')\n",
    "        axs[row_idx, col_idx].set_ylabel('Borda Count')\n",
    "        axs[row_idx, col_idx].legend(methods)\n",
    "        \n",
    "        axs[row_idx, col_idx].set_xticks(covs)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    plt.suptitle(f'n={n} d={d} k={k}', fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Borda\", \"hitting_rate_1\", \"hitting_rate_1_10\", \"ratio\"]:\n",
    "    plot_result(metric=metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "influence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
